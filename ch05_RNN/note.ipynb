{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe06f54",
   "metadata": {},
   "source": [
    "# Chapter 5. Recurrent Neural Network (RNN)\n",
    "\n",
    "Word2vec 만으로는 자연어 처리가 원만하게 이루어질 수 없다! \n",
    "\n",
    "- CBOW (continuous bag-of-words)는 주변 단어를 순서에 무관하게 고려하여 학습하는 단어 분산 모델.\n",
    "- Language model (언어 모델) : 단어의 시퀀스가 일어날 가능성이 어느 정도인지 확률로 평가.<br>\n",
    "m개의 단어 시퀀스 $\\{w_1, w_2, \\cdots, w_m\\}$에 대하여\n",
    "\\begin{equation}\n",
    "P(w_1, w_2, \\cdots, w_m) = \\prod^{m}_{t=1}{P(w_t|w_1,\\cdots,w_{t-1})} .\n",
    "\\end{equation}\n",
    "\n",
    "- 위처럼, 단어의 시퀀스에 따라 확률이 정의되기 때문에, 이를 고려한 확률 모델을 형성할 필요가 있음.\n",
    "\n",
    "## 1. RNN, 순환신경망\n",
    "\n",
    "- 시계열 데이터 $(x_0, x_1, \\cdots, x_t, \\cdots)$ 를 학습하는 데 용이한 모델. ($t$ : 시간?)\n",
    "- $x_t$ -> RNN layer -> $h_t$, $h_t$가 layer에 다시 들어가는 구조.\n",
    "\\begin{equation}\n",
    "h_t = \\tanh{(h_{t-1} W_h + x_t W_x + b)} .\n",
    "\\end{equation}\n",
    "\n",
    "- 위 식과 같이, Input에 대응하는 weight $W_x$와 이전 output에 대응하는 $W_h$가 존재함.\n",
    "- $h_t$는 다음 계층으로 넘어감과 동시에 다음 시각 $t+1$의 RNN 계층으로도 넘어간다.\n",
    "- $h_{t-1}$ 상태에서 $h_t$로 갱신(업데이트)된다고 볼 수 있음. (Memory?)\n",
    "- $h$ : hidden state, or hidden state vector. \n",
    "\n",
    "<hr>\n",
    "\n",
    "### (1) BPTT (BackPropagation Through Time)\n",
    "\n",
    "- BPTT : RNN의 경우, 시간에 따라 RNN 계층으로 이전 시간의 값이 들어오기 때문에, <br>\n",
    "    이 흐름을 따라서도 역전파법으로 파라미터 (weight, bias)를 학습할 수 있다.  \n",
    "- 하지만, 긴 시계열의 경우\n",
    "    - 계산량이 너무 많아지게 되는 문제가 생긴다.\n",
    "    - 역전파시의 기울기가 불안정해지는 문제가 생긴다. (Vanishing gradient, Overflowing, ...)\n",
    "\n",
    "### (2) Truncated BPTT\n",
    "\n",
    "- 다루기 힘든 큰 시계열의 경우 시간축으로 너무 길어진 신경망을 적당한 길이에서 잘라내어 역전파법을 수행.\n",
    "- Example : $x_0$ ~ $x_9$, $x_10$ ~ $x_19$ 식으로 10개 단위로 끊어서 역전파 시행. (RNN이 10개 단위로 학습하게끔 함.)\n",
    "    - cf 1) 순전파는 그대로 진행함에 유의할 것.\n",
    "    - cf 2) 시계열 데이터 $(x_0, x_1, \\cdots, x_t, \\cdots)$ 는 시간 순서대로 입력되어야 함. \n",
    "      (mini-batch 학습은 무작위였음에 유의.)\n",
    "- 위의 예시에 대한 학습 순서 : $x_0$ ~ $x_9$ 에 대해 순전파 및 역전파 시행 ->\n",
    "    $h_9$를 입력받아, $x_{10}$ ~ $x_{19}$ 에 대해 순전파 및 역전파 시행. ($h_9$에 대한 기울기는 계산 X) -> ...\n",
    "    \n",
    "<hr>\n",
    "\n",
    "### (3) Mini-batch 학습\n",
    "- Mini-batch는 무작위로 뽑되, 연속된 데이터 셋을 뽑는다. \n",
    "(ex: 500번째 데이터부터 시작할 경우, ($x_{500}$, $x_{501}$, $\\cdots$) ) \n",
    "- 각 batch 별로 데이터 제공 시작 위치를 옮겨줘야 함. (1번째가 $x_0$ -> $x_{10}$ 이면 2번째는 $x_{500}$ -> $x_{510}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26056121",
   "metadata": {},
   "source": [
    "## 2. RNN 구현\n",
    "\n",
    "$x_s = (x_0, x_1, \\cdots, x_{T-1})$ 의 시계열 입력값들에 대한 출력 $h_s=(h_0,h_1,\\cdots,h_{T-1})$과 같은 구조를\n",
    "하나의 계층이라고 하자.\n",
    "\n",
    "- $N$ : mini-batch size\n",
    "- $D$ : Input dimension\n",
    "- $H$ : layer neuron size\n",
    "- $W_h$, $W_x$ : weight matrix ($h$와 $x$ 각각에 대한)\n",
    "- $ \\tanh{(h_{t-1} W_h + x_{t} W_x + b)} = h_t $ 에 대하여\n",
    "    - $h_{t-1}$, $h_t$ : $N \\times H$ matrix\n",
    "    - $W_h$ : $H \\times H$, square matrix (!!)\n",
    "    - $W_x$ : $N \\times D$\n",
    "    - $b$ : $1 \\times H$ 인데... 각 배치별로 더해주는 거니까, $N \\times H$로 생각하자 (같은 행이 N개 나열된 구조). cf) Repeat 노드, 1.3.4 절 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c144bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 단일 RNN class : 이전 time의 값을 받아 1회 순전파 및 역전파를 하는 class.\n",
    "# class RNN:\n",
    "#     def __init__(self, Wx, Wh, b):\n",
    "#         self.params = [Wx, Wh, b]\n",
    "#         self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "#         self.cache = None\n",
    "        \n",
    "#     # Forward propagation\n",
    "#     # h_prev, 전 시간대의 입력을 받는다.\n",
    "#     def forward(self, x, h_prev):\n",
    "#         Wx, Wh, b = self.params\n",
    "#         t = np.matmul(x,Wx) + np.matmul(h_prev,Wh) + b\n",
    "#         h_next = np.tanh(t)\n",
    "        \n",
    "#         self.cache = (x, h_prev, h_next)\n",
    "#         return h_next\n",
    "    \n",
    "#     # backward propagation, 이후 시간대에 대한 기울기를 입력받음.\n",
    "#     # Output : Input 및 h_prev에 대한 기울기를 반환 (이전 layers로 backpropagation을 위해 넘겨줌)\n",
    "#     def backward(self, dh_next):\n",
    "#         Wx, Wh, b = self.params\n",
    "#         x, h_prev, h_next = self.cache\n",
    "        \n",
    "#         dt = dh_next * (1 - h_next ** 2) # tanh의 미분\n",
    "#         db = np.sum(dt, axis=0)\n",
    "#         dWh = np.matmul(h_prev.T, dt)\n",
    "#         dh_prev = np.matmul(dt, Wh.T)\n",
    "#         dWx = np.matmul(x.T, dt)\n",
    "#         dx = np.matmul(dt, Wx.T)\n",
    "        \n",
    "#         self.grads[0][...] = dWx\n",
    "#         self.grads[1][...] = dWh\n",
    "#         self.grads[2][...] = db\n",
    "        \n",
    "#         return dx, dh_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d933c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TimeRNN 계층 구현\n",
    "# # T개의 RNN 계층으로 구성됨. (T : 임의의 값. 예시에서는 10)\n",
    "# # 다음 T개 sequence에 넘겨줄 (그리고 역전파에서 넘겨받을) 은닉 상태 h를 인스턴스 변수로 저장.\n",
    "\n",
    "# class TimeRNN:\n",
    "#     '''\n",
    "#     parameters\n",
    "    \n",
    "#     stateful : 은닉상태 저장 여부. \n",
    "#         -True:  T번의 시간축 전파 후 은닉상태를 저장함.\n",
    "#         -False : 은닉상태를 저장하지 않음. (h=0 초기화됨.)\n",
    "    \n",
    "#     '''\n",
    "#     def __init__(self, Wx, Wh, b, stateful=False):\n",
    "#         self.params = [Wx, Wh, b]\n",
    "#         self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "#         # self.layers : 다수의 RNN Layer를 저장하기 위한 인스턴스 변수\n",
    "#         self.layers = None\n",
    "        \n",
    "#         # Hidden state and its gradient\n",
    "#         # Truncated BPTT이기에, 여기서 이전시간 역전파에 해당되는 dh는 쓰이지 않지만,\n",
    "#         # 7장 seq2seq에 활용하기 위해 여기서는 일단 저장한다.\n",
    "#         self.h, self.dh = None, None\n",
    "#         self.stateful = stateful\n",
    "        \n",
    "#     def set_state(self, h):\n",
    "#         self.h = h\n",
    "        \n",
    "#     def reset_state(self):\n",
    "#         self.h = None\n",
    "        \n",
    "#     def forward(self, xs):\n",
    "#         Wx, Wh, b = self.params\n",
    "#         # N : mini-batch size\n",
    "#         # T : Number of RNN layers.\n",
    "#         # D : Input data vector dimension\n",
    "#         # H : Number of hidden nodes\n",
    "#         N, T, D = xs.shape\n",
    "#         D, H = Wx.shape\n",
    "        \n",
    "#         self.layers = []\n",
    "#         hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "#         # 처음 전파되는 경우 또는 stateful=False일 경우,\n",
    "#         # h를 0으로 초기화.\n",
    "#         if not self.stateful or self.h is None:\n",
    "#             self.h = np.zeros((N,H), dtype='f')\n",
    "            \n",
    "#         for t in range(T):\n",
    "#             layer = RNN(*self.params)\n",
    "#             self.h = layer.forward(xs[:,t,:], self.h)\n",
    "#             hs[:,t,:] = self.h\n",
    "#             self.layers.append(layer)\n",
    "            \n",
    "#         return hs\n",
    "    \n",
    "#     def backward(self, dhs):\n",
    "#         Wx, Wh, b = self.params\n",
    "#         N, T, H = dhs.shape\n",
    "#         D, H = Wx.shape\n",
    "        \n",
    "#         dxs = np.empty((N,T,D), dtype='f')\n",
    "#         # Truncated BPTT이기에 미래계층에서 역전파한 기울기 dh는 0으로 초기화된다.\n",
    "#         dh = 0\n",
    "#         grads = [0,0,0]\n",
    "        \n",
    "#         # 시간에 역순으로 역전파가 이뤄짐에 주의.\n",
    "#         for t in reversed(range(T)):\n",
    "#             layer = self.layers[t]\n",
    "#             dx, dh = layer.backward(dhs[:, t, :] + dh) # 미래 계층에서의 기울기와 위(뒤) 계층에서의 기울기가 합산되어 전파됨.\n",
    "#             dxs[:, t, :] = dx\n",
    "            \n",
    "#             # 각 RNN 계층에서의 gradient 합산\n",
    "#             for i, grad in enumerate(layer.grads):\n",
    "#                 grads[i] += grad\n",
    "            \n",
    "#         for i, grad in enumerate(grads):\n",
    "#             self.grads[i][...] = grad\n",
    "#         self.dh = dh\n",
    "        \n",
    "#         return dxs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efcf1a",
   "metadata": {},
   "source": [
    "## 3. RNN Language Model (RNNLM)\n",
    "\n",
    "가장 단순한(?) RNNLM의 신경망 구성 : <br>\n",
    "input - Embedding - RNN - Affine - Softmax - output\n",
    "- Embedding : 단어 ID를 단어 벡터로 변환 (word2vec 참고)\n",
    "- RNN : RNN layer (위에 설명됨.)\n",
    "- Affine : Fully-connected layer x*W + b 연산 처리\n",
    "- Softmax : Softmax 함수로 처리.\n",
    "\n",
    "- RNN에서의 hidden state는 이전 sequence의(=과거의) 정보를 '기억'하고 있다. ($h$의 형태로 저장)\n",
    "- T개 씩 끊어서 학습이 되기 때문에, T개에 대한 Loss function의 평균이 계산된다.\n",
    "- T개의 시계열 시퀀스를 처리하는 계층에 대해서는 'common/time_layers.py'를 참고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff19f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple RNN Language Model 구현\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# import numpy as np\n",
    "# from common.time_layers import *\n",
    "\n",
    "# class SimpleRnnlm:\n",
    "#     '''\n",
    "#     Embedding - RNN - Affine - Softmax - Loss\n",
    "#     '''\n",
    "#     def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "#         V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "#         rn = np.random.rand\n",
    "        \n",
    "#         # Xavier initialization\n",
    "#         embed_W = (rn(V,D) / 100).astype('f')\n",
    "#         rnn_Wx = (rn(D,H) / np.sqrt(D)).astype('f')\n",
    "#         rnn_Wh = (rn(H,H) / np.sqrt(H)).astype('f')\n",
    "#         rnn_b = np.zeros(H).astpye('f')\n",
    "#         affine_W = (rn(H,V) / np.sqrt(H)).astype('f')\n",
    "#         affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "#         self.layers = [\n",
    "#             TimeEmbedding(embed_W),\n",
    "#             TimeRNN(rnn_Wx, rnn_Wh,rnn_b,stateful=True)\n",
    "#             TimeAffine(affine_W,affine_b)\n",
    "#         ]\n",
    "#         self.loss_layer = TimeSoftmasWithLoss()\n",
    "#         self.rnn_layer = self.layers[1]\n",
    "        \n",
    "#         self.params, self.grads = [], []\n",
    "#         for layer in layers:\n",
    "#             self.params += layer.params\n",
    "#             self.grads += layer.grads\n",
    "            \n",
    "#     def forward(self, xs, ts):\n",
    "#         for layer in self.layers:\n",
    "#             xs = layer.forward(xs)\n",
    "#         loss = self.loss_layer.forward(xs,ts)\n",
    "#         return loss\n",
    "    \n",
    "#     def backward(self, dout=1):\n",
    "#         dout = self.loss_layer.backward(dout)\n",
    "#         for layer in reversed(self.layers):\n",
    "#             dout = layer.backward(dout)\n",
    "#         return dout\n",
    "    \n",
    "#     def reset_state(self):\n",
    "#         self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2788d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_size: 1000, vocab_size: 418\n",
      "| epoch 1 | perplexity 406.74\n",
      "| epoch 2 | perplexity 304.50\n",
      "| epoch 3 | perplexity 231.41\n",
      "| epoch 4 | perplexity 218.40\n",
      "| epoch 5 | perplexity 207.13\n",
      "| epoch 6 | perplexity 203.55\n",
      "| epoch 7 | perplexity 200.02\n",
      "| epoch 8 | perplexity 196.99\n",
      "| epoch 9 | perplexity 191.33\n",
      "| epoch 10 | perplexity 191.86\n",
      "| epoch 11 | perplexity 187.94\n",
      "| epoch 12 | perplexity 192.01\n",
      "| epoch 13 | perplexity 189.78\n",
      "| epoch 14 | perplexity 190.46\n",
      "| epoch 15 | perplexity 188.65\n",
      "| epoch 16 | perplexity 184.75\n",
      "| epoch 17 | perplexity 182.64\n",
      "| epoch 18 | perplexity 179.38\n",
      "| epoch 19 | perplexity 182.52\n",
      "| epoch 20 | perplexity 184.55\n",
      "| epoch 21 | perplexity 180.43\n",
      "| epoch 22 | perplexity 176.36\n",
      "| epoch 23 | perplexity 172.82\n",
      "| epoch 24 | perplexity 175.35\n",
      "| epoch 25 | perplexity 173.46\n",
      "| epoch 26 | perplexity 170.54\n",
      "| epoch 27 | perplexity 170.90\n",
      "| epoch 28 | perplexity 166.39\n",
      "| epoch 29 | perplexity 167.14\n",
      "| epoch 30 | perplexity 158.65\n",
      "| epoch 31 | perplexity 161.89\n",
      "| epoch 32 | perplexity 154.33\n",
      "| epoch 33 | perplexity 154.73\n",
      "| epoch 34 | perplexity 150.02\n",
      "| epoch 35 | perplexity 147.90\n",
      "| epoch 36 | perplexity 144.50\n",
      "| epoch 37 | perplexity 141.04\n",
      "| epoch 38 | perplexity 136.41\n",
      "| epoch 39 | perplexity 133.31\n",
      "| epoch 40 | perplexity 126.39\n",
      "| epoch 41 | perplexity 127.65\n",
      "| epoch 42 | perplexity 118.39\n",
      "| epoch 43 | perplexity 113.40\n",
      "| epoch 44 | perplexity 108.10\n",
      "| epoch 45 | perplexity 104.52\n",
      "| epoch 46 | perplexity 105.84\n",
      "| epoch 47 | perplexity 99.99\n",
      "| epoch 48 | perplexity 95.33\n",
      "| epoch 49 | perplexity 90.51\n",
      "| epoch 50 | perplexity 87.58\n",
      "| epoch 51 | perplexity 82.75\n",
      "| epoch 52 | perplexity 80.49\n",
      "| epoch 53 | perplexity 73.74\n",
      "| epoch 54 | perplexity 71.32\n",
      "| epoch 55 | perplexity 68.04\n",
      "| epoch 56 | perplexity 65.04\n",
      "| epoch 57 | perplexity 61.27\n",
      "| epoch 58 | perplexity 58.92\n",
      "| epoch 59 | perplexity 55.10\n",
      "| epoch 60 | perplexity 51.65\n",
      "| epoch 61 | perplexity 49.63\n",
      "| epoch 62 | perplexity 47.41\n",
      "| epoch 63 | perplexity 43.70\n",
      "| epoch 64 | perplexity 41.01\n",
      "| epoch 65 | perplexity 39.78\n",
      "| epoch 66 | perplexity 37.33\n",
      "| epoch 67 | perplexity 35.55\n",
      "| epoch 68 | perplexity 32.72\n",
      "| epoch 69 | perplexity 31.13\n",
      "| epoch 70 | perplexity 29.95\n",
      "| epoch 71 | perplexity 27.90\n",
      "| epoch 72 | perplexity 26.38\n",
      "| epoch 73 | perplexity 25.11\n",
      "| epoch 74 | perplexity 23.40\n",
      "| epoch 75 | perplexity 22.48\n",
      "| epoch 76 | perplexity 21.15\n",
      "| epoch 77 | perplexity 20.41\n",
      "| epoch 78 | perplexity 19.02\n",
      "| epoch 79 | perplexity 17.92\n",
      "| epoch 80 | perplexity 16.73\n",
      "| epoch 81 | perplexity 15.73\n",
      "| epoch 82 | perplexity 15.57\n",
      "| epoch 83 | perplexity 13.98\n",
      "| epoch 84 | perplexity 13.40\n",
      "| epoch 85 | perplexity 12.97\n",
      "| epoch 86 | perplexity 12.20\n",
      "| epoch 87 | perplexity 11.72\n",
      "| epoch 88 | perplexity 11.57\n",
      "| epoch 89 | perplexity 10.19\n",
      "| epoch 90 | perplexity 9.83\n",
      "| epoch 91 | perplexity 9.39\n",
      "| epoch 92 | perplexity 8.83\n",
      "| epoch 93 | perplexity 8.61\n",
      "| epoch 94 | perplexity 8.21\n",
      "| epoch 95 | perplexity 7.70\n",
      "| epoch 96 | perplexity 7.27\n",
      "| epoch 97 | perplexity 6.84\n",
      "| epoch 98 | perplexity 6.56\n",
      "| epoch 99 | perplexity 6.24\n",
      "| epoch 100 | perplexity 5.94\n"
     ]
    }
   ],
   "source": [
    "# RNNLM 학습\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.optimizer import SGD, Adam\n",
    "from dataset import ptb\n",
    "from simple_rnnlm import SimpleRnnlm\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100 # RNN hidden nodes\n",
    "time_size = 5\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus)+1)\n",
    "\n",
    "xs = corpus[:-1] # input\n",
    "ts = corpus[1:] # output\n",
    "data_size = len(xs)\n",
    "print('corpus_size: %d, vocab_size: %d' % (corpus_size, vocab_size))\n",
    "\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []\n",
    "\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "# 각 미니배치에서 샘플을 읽기 시작할 위치를 계산.\n",
    "# batch_size 단위로 \n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        batch_x = np.empty((batch_size,time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size,time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "            \n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f' %(epoch+1,ppl))\n",
    "    ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240d5ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(ppl_list))\n",
    "\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('Iteration (x' + str(max_iters) + ')')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3e06c",
   "metadata": {},
   "source": [
    "Note : 위의 훈련 및 그림 코드는 train.py 파일과 동일함.\n",
    "단, RnnlmTrainer class를 사용했는데, 자세한 것은 common/trainer.py 코드 참고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5020f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
